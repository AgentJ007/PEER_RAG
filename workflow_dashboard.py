import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np
from datetime import datetime

# ============================================================================
# PAGE CONFIG
# ============================================================================

st.set_page_config(
    page_title="PeerJ Predictor - Thesis Workflow",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
    <style>
    .main-header {
        font-size: 2rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 1rem;
    }
    .phase-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 0.8rem;
        color: white;
        margin: 0.5rem 0;
    }
    .citation-box {
        background-color: #f0f8ff;
        padding: 1rem;
        border-left: 4px solid #1f77b4;
        border-radius: 0.4rem;
        margin: 0.5rem 0;
        font-size: 0.9rem;
    }
    .research-gap {
        background-color: #fff8e1;
        padding: 1.2rem;
        border-radius: 0.5rem;
        border-left: 5px solid #ff9800;
        margin: 1rem 0;
    }
    .methodology-box {
        background-color: #e8f5e9;
        padding: 1.2rem;
        border-radius: 0.5rem;
        border-left: 5px solid #4caf50;
        margin: 1rem 0;
    }
    .challenge-box {
        background-color: #ffebee;
        padding: 1.2rem;
        border-radius: 0.5rem;
        border-left: 5px solid #f44336;
        margin: 1rem 0;
    }
    .timeline-item {
        padding: 1rem;
        margin: 0.5rem 0;
        border-left: 4px solid #1f77b4;
        background-color: #f5f5f5;
    }
    </style>
""", unsafe_allow_html=True)

# ============================================================================
# SIDEBAR NAVIGATION
# ============================================================================

with st.sidebar:
    st.markdown("# üéì Thesis Workflow")
    st.markdown("---")
    
    page = st.radio(
        "Navigate to:",
        options=[
            "üéØ Overview",
            "üìö Literature & Gap",
            "üî¨ Methodology",
            "üìä Workflow Timeline",
            "‚ö†Ô∏è Challenges & Solutions",
            "üìà Expected Outcomes",
            "üìã Evaluation Plan"
        ]
    )
    
    st.markdown("---")
    
    # Project Info
    st.subheader("Project Details")
    st.info("""
    **Title**: Automated Peer Review Decision Prediction using NLP, RAG, and Statistical Modeling
    
    **Duration**: 24 weeks (6 months)
    
    **Status**: Pre-implementation (Pitch Phase)
    
    **Data**: 250-300 PeerJ papers
    
    **Expected Accuracy**: 75-80%
    """)

# ============================================================================
# PAGE 1: OVERVIEW
# ============================================================================

if page == "üéØ Overview":
    st.markdown('<div class="main-header">üéØ Project Overview & Vision</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    ## Problem Statement
    
    Peer review for scientific manuscripts is:
    - **Time-consuming**: Takes 3-6 months from submission to decision
    - **Inconsistent**: Same paper gets different reviews from different reviewers (inter-rater reliability ~0.6)
    - **Opaque**: Authors don't know WHY their papers are rejected
    - **Biased**: Subject to reviewer expertise variance, personal preferences, and institutional biases
    
    **Current state**: No systematic, data-driven framework exists to predict manuscript quality or 
    review outcomes before submission.
    """)
    
    st.markdown("---")
    
    st.markdown("""
    ## Proposed Solution
    
    Build an **integrated ML system** that combines:
    
    1. **Statistical Modeling** (15 quantitative features from text)
    2. **NLP** (semantic understanding of manuscript content)
    3. **RAG** (retrieval of similar papers to provide evidence)
    4. **Ensemble Methods** (combining multiple models for robustness)
    
    **Output**: Prediction of peer review decision (Accept/Minor/Major) with:
    - Interpretable explanations
    - Actionable recommendations
    - Retrieved evidence from similar papers
    - Confidence intervals
    """)
    
    st.markdown("---")
    
    # Key Statistics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Training Papers", "250-300", help="PeerJ public health papers with complete review histories")
    
    with col2:
        st.metric("Features", "15-17", help="Quantitative features extracted from manuscripts")
    
    with col3:
        st.metric("Models", "3", help="Ordinal Logistic, k-NN, RAG + Ensemble")
    
    with col4:
        st.metric("Target Accuracy", "75-80%", help="Based on similar systems in literature")
    
    st.markdown("---")
    
    # Innovation
    st.markdown("## üöÄ Innovation & Novelty")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### What's NEW in this approach?
        
        ‚úì **First integration of RAG for peer review prediction**
        - Previous work: Classification only
        - This work: Classification + Evidence retrieval + Weighting
        
        ‚úì **Ordinal regression for ordered decisions**
        - Treats Accept < Minor < Major as ordered, not categorical
        - More statistically appropriate
        
        ‚úì **Multi-modal feature engineering**
        - Not just readability or citations
        - Combines statistical rigor, design transparency, ethics
        
        ‚úì **Calibrated uncertainty quantification**
        - Bootstrap confidence intervals
        - Honest about when model is uncertain
        """)
    
    with col2:
        st.markdown("""
        ### Practical Impact
        
        **For Authors**:
        - Early feedback before expensive revisions
        - Identify specific weaknesses
        - Improve manuscript quality systematically
        
        **For Journals**:
        - Data-driven editorial desk-reject decisions
        - Identify reviewer bias
        - Optimize review process
        
        **For Research Community**:
        - Blueprint for similar applications
        - Open-source code + dataset
        - Advance in scientific publishing automation
        """)
    
    st.markdown("---")
    
    # Comparison
    st.subheader("How This Compares to Prior Work")
    
    comparison = pd.DataFrame({
        'Aspect': [
            'Prediction Task',
            'Feature Engineering',
            'Evidence Retrieval',
            'Model Transparency',
            'Uncertainty Quantification',
            'Study Domain'
        ],
        'Prior Work (Classifier)': [
            'Binary classification (Accept/Reject)',
            'Readability, citations, length',
            'No retrieval',
            'Black box (neural networks)',
            'Point predictions only',
            'General computer science'
        ],
        'This Work (Proposed)': [
            'Ordinal classification (3 classes)',
            '15 features: stats, design, ethics, etc.',
            'RAG: retrieve similar papers',
            'Interpretable (logistic regression)',
            'Bootstrap CIs on probabilities',
            'Public health (PeerJ)'
        ]
    })
    
    st.dataframe(comparison, use_container_width=True, hide_index=True)

# ============================================================================
# PAGE 2: LITERATURE & RESEARCH GAP
# ============================================================================

elif page == "üìö Literature & Gap":
    st.markdown('<div class="main-header">üìö Literature Review & Research Gap</div>', 
                unsafe_allow_html=True)
    
    # Background
    st.subheader("Background: Peer Review in Scientific Publishing")
    
    col1, col2 = st.columns([1.5, 1])
    
    with col1:
        st.markdown("""
        ### The Peer Review System
        
        Peer review is the quality assurance mechanism for scientific publishing:
        - Authors submit manuscripts
        - Editors assign 2-3 expert reviewers
        - Reviewers provide feedback and recommendation
        - Editor makes final decision (Accept/Reject/Revise)
        - Authors revise and resubmit if needed
        
        ### Current Challenges
        
        **Consistency Problem**:
        - Inter-rater reliability between reviewers: 0.45-0.65
        - Same paper receives different decisions from different reviewers
        - No standardized quality criteria
        
        **Delay Problem**:
        - Average review time: 4-6 months
        - Publication lag: 12-24 months from submission to publication
        
        **Bias Problem**:
        - Reviewer expertise varies widely
        - Institutional biases
        - Gender/nationality biases documented in literature
        """)
    
    with col2:
        st.markdown("""
        ### Key Statistics
        
        üìä **PeerJ Journal** (Case Study):
        - Founded: 2012
        - Submission rate: ~8,000 papers/year
        - Acceptance rate: ~35-45%
        - Open review policy (optional)
        
        üìä **Global Scientific Publishing**:
        - 3+ million papers published annually
        - 10+ million review requests sent
        - 2-4 weeks average review time
        - ~15% of papers desk-rejected
        """)
    
    st.markdown("---")
    
    # Literature Review Sections
    st.subheader("üîç Key Research Areas")
    
    tabs = st.tabs([
        "Peer Review Analysis",
        "ML for Academic Text",
        "RAG Systems",
        "Ordinal Regression",
        "Research Gap"
    ])
    
    # Tab 1: Peer Review Analysis
    with tabs[0]:
        st.markdown("""
        ### Peer Review Analysis Studies
        
        **Fundamental Research**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>Cicchetti, D. V. (1991).</b> "The reliability of peer review for manuscript and grant submissions: 
        A cross-disciplinary investigation." <i>Journal of the American Academy of Child & Adolescent Psychiatry</i>, 30(3), 431-438.
        <br><br>
        <b>Key Finding</b>: Inter-rater agreement for journal decisions averages 0.45-0.65 depending on field.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="citation-box">
        <b>Pier, E. L., et al. (2018).</b> "Low agreement among reviewers evaluating the same NIH grant 
        applications." <i>PNAS</i>, 115(12), 2952-2957.
        <br><br>
        <b>Key Finding</b>: Even expert reviewers disagree substantially on manuscript/grant quality.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="citation-box">
        <b>Helgesson, G., & Eriksson, S. (2018).</b> "Reporting and investigating peer review fraud." 
        <i>Nature Medicine</i>, 24(8), 1258-1264.
        <br><br>
        <b>Key Finding</b>: Systematic biases exist in peer review; systematic approach could help identify/mitigate.
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 2: ML for Academic Text
    with tabs[1]:
        st.markdown("""
        ### Machine Learning on Scientific Text
        
        **Document Classification**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>Devlin, J., Chang, M. W., et al. (2019).</b> "BERT: Pre-training of Deep Bidirectional Transformers 
        for Language Understanding." <i>NAACL</i>.
        <br><br>
        <b>Relevance</b>: Foundation for modern NLP. BERT-variants (SciBERT, BioBERT) fine-tuned for scientific text.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="citation-box">
        <b>Beltagy, I., Lo, K., & Cohan, A. (2019).</b> "SciBERT: A Pretrained Language Model for Scientific Text." 
        <i>EMNLP</i>.
        <br><br>
        <b>Relevance</b>: Domain-specific embeddings for biomedical/scientific text. Our feature extraction leverages this.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="citation-box">
        <b>Luan, Y., He, L., et al. (2020).</b> "A General Domain-Agnostic Multilingual Meta-Embedding 
        Scheme for Semantic Similarity." <i>ICLR</i>.
        <br><br>
        <b>Relevance</b>: Methods for creating robust embeddings of academic papers. Applicable to our RAG system.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        **Manuscript Quality Assessment**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>Jeckel, S., et al. (2017).</b> "Automatic detection of document structure in noisy digital 
        scientific papers." <i>JCDL</i>.
        <br><br>
        <b>Relevance</b>: Methods for parsing scientific papers; applicable to our feature extraction pipeline.
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 3: RAG Systems
    with tabs[2]:
        st.markdown("""
        ### Retrieval-Augmented Generation
        
        **RAG Fundamentals**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>Lewis, P., Perez, E., et al. (2020).</b> "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." 
        <i>NeurIPS</i>.
        <br><br>
        <b>Key Contribution</b>: Foundational RAG paper. Combines dense retrieval + generation. 
        Our work applies retrieval for weighting decisions (not generation).
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="citation-box">
        <b>Guu, K., Lee, K., et al. (2020).</b> "Retrieval Augmented Language Model Pre-Training." 
        <i>ICML</i>.
        <br><br>
        <b>Relevance</b>: Shows RAG improves performance over standalone models. 
        Our M3 model follows this principle.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        **Vector Databases & Semantic Search**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>Johnson, J., Douze, M., & J√©gou, H. (2019).</b> "Billion-scale similarity search with GPUs." 
        <i>IEEE TPAMI</i>, 43(5), 1701-1710.
        <br><br>
        <b>Relevance</b>: Efficient similarity search at scale. Qdrant implements these algorithms 
        for our 1500-paper corpus.
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 4: Ordinal Regression
    with tabs[3]:
        st.markdown("""
        ### Ordinal Classification Models
        
        **Proportional Odds Models**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>McCullagh, P. (1980).</b> "Regression Models for Ordinal Data." 
        <i>Journal of the Royal Statistical Society</i>, 42(2), 109-142.
        <br><br>
        <b>Foundational Work</b>: Introduced proportional odds model. Still widely used for ordinal outcomes.
        Our M1 model is based on this framework.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="citation-box">
        <b>Agresti, A. (2010).</b> "Analysis of Ordinal Categorical Data" (2nd ed.). 
        <i>Wiley Series in Probability and Statistics</i>.
        <br><br>
        <b>Reference</b>: Comprehensive treatment of ordinal regression. Covers assumption testing, 
        interpretation, and diagnostics.
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        **Applications in NLP/ML**:
        """)
        
        st.markdown("""
        <div class="citation-box">
        <b>Rennie, J. D. M., et al. (2005).</b> "Tackling the Poor Assumptions of Naive Bayes Text 
        Classifiers." <i>ICML</i>.
        <br><br>
        <b>Relevance</b>: Shows ordinal methods improve over nominal classification for text. 
        Motivation for our ordinal approach.
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 5: Research Gap
    with tabs[4]:
        st.markdown("""
        ## Research Gap Analysis
        """)
        
        st.markdown("""
        <div class="research-gap">
        <h3>What EXISTS in Literature:</h3>
        
        ‚úì **Binary peer review prediction** (Accept/Reject) - Stelmakh et al. 2021, Yang et al. 2020
        
        ‚úì **NLP-based manuscript quality assessment** - Using readability, citations, structure
        
        ‚úì **RAG systems** - Lewis et al. 2020, but mostly for QA tasks
        
        ‚úì **Ordinal regression** - Well-established in statistics (McCullagh 1980)
        
        ‚úì **Ensemble methods** - Standard in ML (Breiman 2001, Schapire 2003)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="research-gap" style="background-color: #fff3e0; border-left-color: #ff6f00;">
        <h3>What DOESN'T EXIST (Research Gap):</h3>
        
        ‚úó **Ordinal peer review prediction** - All prior work treats as binary/3-class nominal
        
        ‚úó **RAG for peer review decisions** - Retrieval used for generation/QA, NOT for decision weighting
        
        ‚úó **Integrated statistical + NLP approach** - No work combines both systematically
        
        ‚úó **Calibrated uncertainty in peer review prediction** - Few papers report confidence intervals
        
        ‚úó **Multi-feature statistical profiling** - Most work uses 5-10 features; we propose 15-17
        
        ‚úó **Publicly available peer review prediction dataset** - No large, annotated dataset (except our work)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        st.markdown("""
        ## Why This Matters
        
        **Scientific Contribution**:
        - First to combine ordinal regression + RAG for peer review
        - First to provide interpretable, evidence-based explanations
        - First to report confidence intervals on predictions
        
        **Practical Contribution**:
        - Tool for authors to self-assess manuscripts
        - Dataset + code released for community use
        - Blueprint for other domains (grant review, hiring, etc.)
        
        **Methodological Contribution**:
        - Shows how to properly weight multiple models
        - Demonstrates ordinal regression benefits for decision prediction
        - Example of calibrated uncertainty quantification
        """)

# ============================================================================
# PAGE 3: METHODOLOGY
# ============================================================================

elif page == "üî¨ Methodology":
    st.markdown('<div class="main-header">üî¨ Technical Methodology</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    ## Overview: Three-Pronged Approach
    
    Our system integrates:
    1. **Statistical modeling** (features extracted from text)
    2. **NLP & semantic similarity** (embeddings and retrieval)
    3. **Ensemble integration** (combining multiple models)
    """)
    
    st.markdown("---")
    
    # Methodology Tabs
    methodology_tabs = st.tabs([
        "Feature Engineering",
        "Statistical Model (M1)",
        "RAG System (M3)",
        "Ensemble Integration",
        "Validation Strategy"
    ])
    
    # Tab 1: Features
    with methodology_tabs[0]:
        st.markdown("### Feature Engineering: 15-17 Quantitative Features")
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Block A: Statistical Rigor (5 features)</h4>
        
        1. <b>Statistical Term Density</b>: % of words in Methods that are statistical terms
           - Dictionary: 120 terms (t-test, ANOVA, regression, CI, p-value, etc.)
           - Expected: 2-4% for typical paper
           - Backed by: Readability research (Flesch 1948, Gunning 1952)
        
        2. <b>Named Test Count</b>: How many distinct statistical tests mentioned
           - Examples: t-test, ANOVA, logistic regression, Cox proportional hazards
           - Expected: 6-12 for empirical studies
           - Backed by: STROBE guidelines (Vandenbroucke et al. 2007)
        
        3. <b>Effect Size Reporting</b>: Are effect sizes + CIs provided?
           - Critical for interpretation
           - Backed by: Publication Manual APA 7th ed., Fisher et al. 2019
        
        4. <b>Sample Size Justification</b>: Is power analysis documented?
           - Yes/No indicator
           - Backed by: Power analysis standards (Cohen 1988)
        
        5. <b>Confounding Adjustment</b>: How many confounders addressed?
           - Count: "adjusted for", "controlling for", "stratified"
           - Backed by: Causal inference literature (Rotnitzky & Vansteelandt 2010)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Block B: Study Design Transparency (3 features)</h4>
        
        1. <b>Study Design Explicit</b>: Is design clearly stated?
           - Regex match: "RCT", "cohort", "cross-sectional", "case-control"
           - Backed by: STROBE checklist (22-25 items depending on design)
        
        2. <b>Setting Description</b>: Is study location documented?
           - Keywords: hospital, clinic, community, primary care
        
        3. <b>Eligibility Clarity</b>: Are inclusion/exclusion criteria clear?
           - Sentence count in eligibility section
           - Backed by: CONSORT guidelines (Schulz et al. 2010)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Block C: Readability & Writing (4 features)</h4>
        
        1. <b>Flesch-Kincaid Grade Level</b>: Text complexity
           - Formula: 0.39*(words/sentences) + 11.8*(syllables/words) - 15.59
           - Expected: 12-16 for scientific writing
           - Backed by: Flesch 1948, Kincaid et al. 1975
        
        2. <b>Average Sentence Length</b>: Words per sentence
           - Expected: 15-22 for academic writing
           - Backed by: Gunning Fog Index (Gunning 1952)
        
        3. <b>Passive Voice Ratio</b>: % passive vs active voice
           - NLP-based using spaCy dependency parsing
           - Expected: 30-50% in scientific writing (higher = harder to read)
           - Backed by: Journal style guidelines
        
        4. <b>Vocabulary Richness</b>: Type-token ratio
           - Formula: unique_words / total_words
           - Expected: 0.35-0.50
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Block D: Literature Grounding (4 features)</h4>
        
        1. <b>Citation Count</b>: Total references
           - Expected: 25-60 for empirical studies
        
        2. <b>Citation Recency</b>: Median publication year
           - Expected: 2018-2021 for current research
           - Backed by: Kuhn cycle theory (papers cited 0-5 years old are current)
        
        3. <b>Citation Format Consistency</b>: % using same style
           - Expected: 95%+
        
        4. <b>Self-Citation Ratio</b>: % self-citations
           - Expected: 0-10% (>15% is red flag)
           - Backed by: Research integrity literature (Noyons et al. 2003)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Block E: Document Structure (4 features)</h4>
        
        1. <b>Methods Ratio</b>: % Methods / total body text
           - Expected: 15-35% depending on study type
           - Backed by: IMRaD structure (Sollaci & Pereira 2004)
        
        2. <b>Results Ratio</b>: % Results / total body text
           - Expected: 15-30%
        
        3. <b>Discussion-to-Results Ratio</b>: Discussion length vs Results
           - Expected: 1.0-2.0 (discussion elaborates, not speculates)
        
        4. <b>Abstract Structured</b>: Does abstract have sections?
           - Yes/No (required by most journals)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Block F: Ethics & Transparency (3 features)</h4>
        
        1. <b>Ethics Approval Stated</b>: IRB/Ethics Committee approval mentioned?
           - Required for human subject research
           - Backed by: Declaration of Helsinki (World Medical Association 2013)
        
        2. <b>Data Availability Statement</b>: Is data availability discussed?
           - Backed by: Open Science Framework, PLoS data policy
        
        3. <b>Conflict of Interest Stated</b>: COI statement present?
           - Backed by: ICMJE guidelines (International Committee of Medical Journal Editors 2023)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        st.markdown("""
        ### Validation Strategy for Features
        
        1. **Manual verification** on 20 papers
           - Compare automated extraction vs manual annotation
           - Calculate inter-rater agreement (target: >90%)
        
        2. **Correlation analysis**
           - Identify redundant features (r > 0.80)
           - Calculate VIF for multicollinearity
        
        3. **Normality testing**
           - Shapiro-Wilk test for each feature
           - Document which features are non-normal
        
        4. **Study-type stratification**
           - Calculate feature norms separately for RCTs, Cohort, Cross-sectional
           - Ensures fair evaluation across designs
        """)
    
    # Tab 2: M1 Statistical Model
    with methodology_tabs[1]:
        st.markdown("### Model M1: Ordinal Logistic Regression")
        
        st.markdown("""
        #### Why Ordinal Regression?
        
        **Problem**: Peer review decisions are ORDERED
        - Accept < Minor Revision < Major Revision
        - Can't treat as nominal categories (violates order)
        - Ordinary logistic regression (binary/multinomial) is inappropriate
        
        **Solution**: Proportional odds model (McCullagh 1980)
        - Treats decision as ordered outcome
        - More statistically appropriate
        - Better leverages ordering information
        
        **Backed by**: McCullagh 1980, Agresti 2010, Rennie et al. 2005
        """)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Model Specification</h4>
        
        **Proportional Odds Model**:
        
        log(P(Y ‚â§ j) / P(Y > j)) = Œ±_j - Œ≤^T x
        
        Where:
        - Y ‚àà {Accept, Minor, Major} (coded 0, 1, 2)
        - j ‚àà {0, 1} (two thresholds)
        - Œ≤ = coefficients for each feature
        - Œ±_j = threshold parameters
        - x = 15-17 standardized feature values (Z-scores)
        
        **Output**: P(Accept), P(Minor), P(Major) probabilities
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        #### Implementation Details
        
        1. **Data Preparation**
           - Features normalized to Z-scores (by study type)
           - Study type stratification (separate models if n ‚â• 30)
           - Handle missing: impute with study-type median
        
        2. **Model Fitting**
           - Library: statsmodels.api.ordinal_model (Python)
           - Estimation: Maximum likelihood
           - Train set: 200 papers (80%)
           - Validation set: 50 papers (20%)
        
        3. **Assumption Testing**
           - **Proportional odds**: Brant test (p > 0.05 = assumption holds)
           - **Multicollinearity**: VIF < 5
           - **Model fit**: AIC, BIC, Pseudo-R¬≤
        
        4. **Interpretation**
           - Coefficient Œ≤: Change in log-odds per 1 SD increase in feature
           - Odds ratio: exp(Œ≤) = multiplicative change in odds
           - Example: Œ≤ = -0.82 means 1 SD increase in stat density ‚Üí 0.44√ó odds of worse decision
        
        5. **Cross-validation**
           - 5-fold CV to assess stability
           - Bootstrap standard errors (1000 resamples)
        """)
        
        st.markdown("""
        #### Expected Performance
        
        - **Accuracy**: 70-75% (based on ordinal models in similar domains)
        - **Macro F1**: 0.65-0.72
        - **Calibration**: Good to excellent (Brier score < 0.05)
        - **Inference time**: < 10ms per prediction
        """)
    
    # Tab 3: M3 RAG System
    with methodology_tabs[2]:
        st.markdown("### Model M3: Retrieval-Augmented Generation (RAG)")
        
        st.markdown("""
        #### Why RAG?
        
        **Problem**: Papers with similar statistical/design profiles likely receive similar decisions
        - But M1 alone can't access this information
        - Individual features don't capture holistic patterns
        
        **Solution**: Retrieve similar papers, aggregate their decisions
        - Use semantic similarity to find comparable manuscripts
        - Weight retrieved papers by similarity + statistical distance
        - Aggregate decisions using weighted voting
        
        **Backed by**: Lewis et al. 2020, Guu et al. 2020
        """)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>RAG Pipeline: 6-Stage Retrieval</h4>
        
        **Stage 1: Aspect Decomposition**
        - Decompose new paper into 5 aspects:
          - Design (study design, population)
          - Statistics (test selection, analysis)
          - Confounding (adjustment methods)
          - Data (collection, measurement)
          - Ethics (approvals, consent)
        - Method: Zero-shot text classification (facebook/bart-large-mnli)
        
        **Stage 2: Semantic Retrieval (Bi-Encoder)**
        - For each aspect, retrieve top-50 similar papers
        - Method: S-PubMedBert embeddings + FAISS search
        - Input: Query text (~100-150 words per aspect)
        - Output: 50 candidate papers per aspect
        - Time: ~1-2 seconds
        
        **Stage 3: Cross-Encoder Re-Ranking**
        - Re-rank top-50 using cross-encoder
        - Model: cross-encoder/ms-marco-MiniLM-L-12-v2
        - Input: (query, candidate_text) pairs
        - Output: Relevance score 0-1
        - Keep top-10 per aspect
        - Time: ~1-2 seconds
        
        **Stage 4: Entity-Based Filtering**
        - Extract named entities from new paper:
          - Statistical methods (t-test, regression, etc.)
          - Populations (adults, children, patients, etc.)
          - Diseases (diabetes, cancer, etc.)
          - Measurements (BMI, blood pressure, etc.)
        - Method: SpaCy NER fine-tuned on biomedical text
        - Keep candidates with Jaccard(entities) > 0.30
        - Rationale: Ensures retrieved papers use similar methods
        
        **Stage 5: Statistical Distance Filtering**
        - Calculate Mahalanobis distance (multivariate Z-score)
        - Formula: D = ‚àö[(x - Œº)^T Œ£^(-1) (x - Œº)]
        - Keep papers with D < 3.0 (99% confidence ellipse)
        - Rationale: Ensures papers are statistically similar
        
        **Stage 6: Weighted Aggregation**
        - Weight each paper by:
          - w_i = 1 / (1 + Mahalanobis_distance_i¬≤)
          - Papers closer to training mean get higher weight
        - Aggregate decision: P(decision | papers) = Œ£ w_i √ó I(paper_i ‚Üí decision) / Œ£ w_i
        - Output: Probability distribution over decisions
        
        **Total Time**: ~2-3 seconds for full pipeline
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        #### Why This Approach Works
        
        1. **Semantic + Statistical filtering**: Finds papers that are similar in BOTH meaning AND structure
        2. **Evidence-based weighting**: Closer papers have more influence
        3. **Interpretability**: Can show users which papers influenced decision
        4. **Robustness**: Ensemble of multiple retrievals (per aspect) reduces noise
        
        **Backed by**:
        - Dense retrieval: Johnson et al. 2019
        - Cross-encoder re-ranking: Nogueira et al. 2020
        - Inverse-variance weighting: Standard statistical practice
        """)
    
    # Tab 4: Ensemble
    with methodology_tabs[3]:
        st.markdown("### Ensemble Integration & Weighting")
        
        st.markdown("""
        #### Three-Model Ensemble
        
        We combine 3 models:
        
        | Model | Type | Input | Output |
        |-------|------|-------|--------|
        | **M1** | Ordinal Logistic | 15 Z-score features | P(decision) |
        | **M2** | k-NN (k=10, Mahalanobis) | 15 Z-score features | Majority vote |
        | **M3** | RAG + Weighted Agg | Full paper text | Weighted P(decision) |
        
        **Why ensemble?**
        - Reduces overfitting
        - Combines interpretability (M1) + similarity (M2) + evidence (M3)
        - Better calibration
        - More robust to domain shift
        
        **Backed by**: Breiman 2001, Schapire 2003, Zhou 2012
        """)
        
        st.markdown("""
        <div class="methodology-box">
        <h4>Ensemble Weighting Strategy</h4>
        
        **Method**: Inverse-Variance Weighting via Brier Score
        
        1. **Calculate Brier Scores** on validation set (50 papers):
           - BS_i = (1/n) Œ£ (p_predicted - y_actual)¬≤
           - Lower is better
           - Example: BS_M1 = 0.032, BS_M2 = 0.039, BS_M3 = 0.025
        
        2. **Compute weights**:
           - w_i = 1 / (1 + BS_i)
           - w_M1 = 1/(1+0.032) = 0.969
           - w_M2 = 1/(1+0.039) = 0.962
           - w_M3 = 1/(1+0.025) = 0.976
        
        3. **Normalize**:
           - w_M1_norm = 0.969 / (0.969+0.962+0.976) = 0.333
           - w_M2_norm = 0.333
           - w_M3_norm = 0.336
        
        4. **Final prediction**:
           - P(decision) = Œ£ w_i √ó P_i(decision)
           - Example: P(Major) = 0.333√ó0.65 + 0.333√ó0.70 + 0.336√ó0.72 = 0.691
        
        **Advantage**: Models that are well-calibrated automatically get higher weight
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 5: Validation
    with methodology_tabs[4]:
        st.markdown("### Validation & Evaluation Strategy")
        
        st.markdown("""
        #### Data Partitioning
        
        - **Training set** (80%, n=200): Fit models, choose hyperparameters
        - **Validation set** (20%, n=50): Evaluate performance, calibration, ensemble weights
        - **Cross-validation** (5-fold): Assess stability, get uncertainty estimates
        
        **Stratification**: Ensure decision distribution matches across sets
        """)
        
        st.markdown("""
        #### Evaluation Metrics
        
        **Classification Metrics**:
        - Accuracy: % correctly predicted
        - Precision/Recall per class (Accept, Minor, Major)
        - Macro F1-score: Equal weight per class (target: ‚â•0.70)
        - Confusion matrix: What mistakes does model make?
        
        **Calibration Metrics**:
        - Brier score: (prediction - reality)¬≤ (target: <0.05)
        - Expected calibration error (ECE)
        - Calibration plot: Predicted vs Observed frequencies
        
        **Uncertainty Metrics**:
        - Bootstrap CI coverage: Do 95% CIs contain true value ~95% of time?
        - CI width: Are CIs reasonable width (not too wide/narrow)?
        
        **Interpretability Metrics**:
        - Feature importance (regression coefficients)
        - SHAP values (per-prediction feature attribution)
        - Retrieved paper relevance (manual spot-check)
        """)
        
        st.markdown("""
        #### Validation Procedures
        
        1. **Accuracy Validation**
           - Report metrics separately per decision class
           - Flag if performance imbalanced (e.g., perfect on Major, poor on Accept)
        
        2. **Calibration Validation**
           - Fit calibration curve on validation set
           - Apply Platt scaling if necessary
           - Test coverage: 95% CI should contain truth ~95% of time
        
        3. **Stability Testing**
           - 5-fold cross-validation: Same metrics on each fold
           - Bootstrap resampling (1000√ó): Coefficient estimates should be stable
           - Check: SE(coefficient) should be small relative to coefficient
        
        4. **Feature Validation**
           - Manual verification on 20 papers
           - Check: Do extracted features match manual inspection?
        
        5. **Domain Validation**
           - Test on PeerJ papers NOT used in training
           - Validate study-type specific norms
        
        6. **Adversarial Testing**
           - Short papers (< 2000 words)
           - Unusual formats (non-standard structure)
           - Non-native English (should flag low confidence)
        """)

# ============================================================================
# PAGE 4: WORKFLOW TIMELINE
# ============================================================================

elif page == "üìä Workflow Timeline":
    st.markdown('<div class="main-header">üìä Project Workflow Timeline (24 Weeks)</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    ## High-Level Schedule
    
    **Total Duration**: 24 weeks (6 months)
    **Work Load**: ~50 hours/week, organized in 7 phases
    """)
    
    st.markdown("---")
    
    # Phase cards
    phases = [
        {
            "name": "Phase 1: Foundation & Data",
            "weeks": "Weeks 1-4",
            "color": "#667eea",
            "tasks": [
                "‚úì Literature review (20+ papers)",
                "‚úì Download 300 PeerJ papers",
                "‚úì Extract text from PDFs",
                "‚úì Create structured dataset",
                "‚úì Exploratory data analysis"
            ],
            "deliverable": "Clean dataset (300 papers, metadata)"
        },
        {
            "name": "Phase 2: Feature Engineering",
            "weeks": "Weeks 5-8",
            "color": "#764ba2",
            "tasks": [
                "‚úì Implement feature extractors (15 features)",
                "‚úì Manual validation (20 papers)",
                "‚úì Calculate reference statistics",
                "‚úì Feature correlation analysis",
                "‚úì Study-type stratification"
            ],
            "deliverable": "Feature matrix (250√ó15), codebook"
        },
        {
            "name": "Phase 3: NLP & RAG",
            "weeks": "Weeks 9-11",
            "color": "#f093fb",
            "tasks": [
                "‚úì Generate embeddings (S-PubMedBert)",
                "‚úì Set up Qdrant vector database",
                "‚úì Implement 6-stage retrieval pipeline",
                "‚úì Cross-encoder re-ranking",
                "‚úì Entity filtering (NER)"
            ],
            "deliverable": "RAG system, vector database (1500 embeddings)"
        },
        {
            "name": "Phase 4: Statistical Models",
            "weeks": "Weeks 12-15",
            "color": "#4facfe",
            "tasks": [
                "‚úì Fit ordinal logistic regression (M1)",
                "‚úì Test proportional odds assumption",
                "‚úì Implement k-NN model (M2)",
                "‚úì Bootstrap confidence intervals",
                "‚úì Calibration testing"
            ],
            "deliverable": "Trained M1 & M2 models, validation metrics"
        },
        {
            "name": "Phase 5: Integration",
            "weeks": "Weeks 16-18",
            "color": "#43e97b",
            "tasks": [
                "‚úì Integrate RAG with M1/M2",
                "‚úì Implement ensemble weighting",
                "‚úì Brier score calibration",
                "‚úì Full system testing",
                "‚úì End-to-end prediction pipeline"
            ],
            "deliverable": "Integrated ensemble system"
        },
        {
            "name": "Phase 6: Testing & Validation",
            "weeks": "Weeks 19-20",
            "color": "#fa709a",
            "tasks": [
                "‚úì Unit + integration tests",
                "‚úì Performance testing (latency, memory)",
                "‚úì Error analysis (where does model fail?)",
                "‚úì Case study validation (5 papers)",
                "‚úì Final metrics report"
            ],
            "deliverable": "Test suite (>80% coverage), case studies"
        },
        {
            "name": "Phase 7: Documentation & Thesis",
            "weeks": "Weeks 21-24",
            "color": "#ffa502",
            "tasks": [
                "‚úì Code documentation + API reference",
                "‚úì Write thesis chapters 1-3 (Intro, Lit, Methods)",
                "‚úì Write thesis chapters 4-5 (Results, Discussion)",
                "‚úì Create visualizations & figures",
                "‚úì Final thesis revision + submission"
            ],
            "deliverable": "Complete thesis (15,000+ words), code, presentation"
        }
    ]
    
    for i, phase in enumerate(phases, 1):
        with st.container():
            col1, col2 = st.columns([1, 3])
            
            with col1:
                st.markdown(f"""
                <div class="phase-card" style="background: linear-gradient(135deg, {phase['color']} 0%, rgba(100,100,150,0.8) 100%);">
                <h3 style="margin: 0; font-size: 1.2rem;">{phase['name']}</h3>
                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">{phase['weeks']}</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"**Tasks:**")
                for task in phase['tasks']:
                    st.markdown(f"- {task}")
                
                st.markdown(f"**üì¶ Deliverable**: {phase['deliverable']}")
        
        st.write("")
    
    st.markdown("---")
    
    # Gantt-style visualization
    st.subheader("Gantt Chart (Timeline Visualization)")
    
    gantt_data = pd.DataFrame({
        'Phase': [p['name'].split(':')[0] for p in phases],
        'Start': list(range(1, 25, 3)) + [22],
        'Duration': [4, 4, 3, 4, 3, 2, 4]
    })
    
    gantt_data['End'] = gantt_data['Start'] + gantt_data['Duration']
    
    fig = go.Figure()
    
    colors_gantt = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b', '#fa709a', '#ffa502']
    
    for i, row in gantt_data.iterrows():
        fig.add_trace(go.Bar(
            y=[row['Phase']],
            x=[row['Duration']],
            orientation='h',
            marker=dict(color=colors_gantt[i]),
            name=f"Week {row['Start']}-{row['End']}",
            text=f"W{row['Start']}-{row['End']}",
            textposition='auto',
            hovertemplate=f"{row['Phase']}<br>Week {row['Start']}-{row['End']}<extra></extra>"
        ))
    
    fig.update_layout(
        title='Project Timeline: 24 Weeks',
        xaxis_title='Weeks',
        yaxis_title='',
        barmode='overlay',
        height=400,
        showlegend=False,
        template='plotly_white',
        xaxis=dict(range=[0, 24])
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    # Key Milestones
    st.subheader("Key Milestones & Decision Points")
    
    milestones = [
        {
            'week': 4,
            'title': 'Data Quality Check',
            'decision': 'Go/No-Go: Is extracted text quality acceptable?',
            'criteria': '>95% extraction success rate'
        },
        {
            'week': 8,
            'title': 'Feature Validation',
            'decision': 'Go/No-Go: Do features work as expected?',
            'criteria': '>90% agreement between automated & manual extraction'
        },
        {
            'week': 12,
            'title': 'Model Baseline',
            'decision': 'Go/No-Go: Does M1 achieve ‚â•70% accuracy?',
            'criteria': 'Accuracy ‚â•70%, no major assumption violations'
        },
        {
            'week': 16,
            'title': 'Ensemble Integration',
            'decision': 'Go/No-Go: Does ensemble improve over baseline?',
            'criteria': 'Ensemble accuracy ‚â•M1 baseline'
        },
        {
            'week': 20,
            'title': 'Final Validation',
            'decision': 'Go/No-Go: Does system meet acceptance criteria?',
            'criteria': 'Accuracy ‚â•75%, Macro F1 ‚â•0.70, Brier <0.05'
        }
    ]
    
    for milestone in milestones:
        col1, col2, col3 = st.columns([1, 2, 2])
        
        with col1:
            st.markdown(f"**Week {milestone['week']}**")
        
        with col2:
            st.markdown(f"**{milestone['title']}**")
            st.caption(milestone['decision'])
        
        with col3:
            st.success(f"‚úì {milestone['criteria']}")

# ============================================================================
# PAGE 5: CHALLENGES & SOLUTIONS
# ============================================================================

elif page == "‚ö†Ô∏è Challenges & Solutions":
    st.markdown('<div class="main-header">‚ö†Ô∏è Challenges & Mitigation Strategies</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    Implementing this system will face real technical and conceptual challenges.
    Here's how we'll address each:
    """)
    
    st.markdown("---")
    
    challenge_tabs = st.tabs([
        "Data Quality",
        "Feature Engineering",
        "Statistical Issues",
        "NLP & Retrieval",
        "Integration",
        "Timeline Risk"
    ])
    
    # Tab 1: Data Quality
    with challenge_tabs[0]:
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: PDF Extraction Errors</h4>
        
        <b>Problem</b>: PDFs vary in structure. Some are scanned (OCR required), others corrupted.
        
        <b>Impact</b>: ~15-20% may have extraction errors ‚Üí biased training data
        
        <b>Mitigation</b>:
        1. Use OCR (Tesseract) for scanned PDFs, flag if confidence <85%
        2. Manual verification of extracted text (10% of papers)
        3. Create fallback: Use abstract + metadata if full text fails
        4. Document extraction quality metric per paper
        
        <b>Success Metric</b>: ‚â•95% extraction success rate
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Missing Review Data</h4>
        
        <b>Problem</b>: Some PeerJ papers don't have visible review comments (author anonymity setting).
        
        <b>Impact</b>: ~10% may lack detailed reviewer feedback for M3 RAG weighting
        
        <b>Mitigation</b>:
        1. Use only papers with complete decision information
        2. If reviews missing but decision available: Still use (decision is enough)
        3. RAG retrieval works without sentiment analysis
        
        <b>Success Metric</b>: ‚â•250 papers with complete data
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Decision Class Imbalance</h4>
        
        <b>Problem</b>: Likely skew (60% Major, 25% Accept, 15% Minor) in PeerJ data
        
        <b>Impact</b>: Model biased toward predicting majority class
        
        <b>Mitigation</b>:
        1. Use class weights in ordinal regression
        2. Stratified train/validation split
        3. Report Macro F1 (equal weight per class)
        4. Bootstrap oversample minority classes
        5. Monitor precision/recall per class separately
        
        <b>Success Metric</b>: F1 ‚â•0.70 across all three classes
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 2: Feature Engineering
    with challenge_tabs[1]:
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Ambiguous Feature Definitions</h4>
        
        <b>Problem</b>: What counts as "statistical test"? Does "correlation" count if not named "Pearson"?
        
        <b>Impact</b>: Inconsistent extraction; validation disagreement
        
        <b>Mitigation</b>:
        1. Create detailed feature codebook with 20 examples per feature
        2. Test extraction on 20 papers ‚Üí manual review ‚Üí resolve discrepancies
        3. Create clear regex patterns + keyword lists (frozen after validation)
        4. Document edge cases explicitly
        5. Inter-rater agreement target: >90%
        
        <b>Success Metric</b>: Cohen's kappa >0.90 on validation sample
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Study-Type-Specific Features</h4>
        
        <b>Problem</b>: Some features don't apply to all study types.
        "Confounding adjustment" is critical for observational, irrelevant for RCTs.
        
        <b>Impact</b>: Non-comparable feature distributions across types
        
        <b>Mitigation</b>:
        1. Build study-type-specific feature subsets
        2. Create indicator: "applies_to_study_type" (1/0)
        3. Build separate models per study type (if n‚â•30)
        4. Use interaction features: feature √ó study_type
        5. Normalize Z-scores within each study type
        
        <b>Success Metric</b>: Accuracy comparable across RCT, Cohort, Cross-sectional (¬±3%)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Feature Extraction at Scale</h4>
        
        <b>Problem</b>: 15 features √ó 300 papers = 4500 calculations. One bug affects all.
        
        <b>Impact</b>: Systematic errors propagate through entire system
        
        <b>Mitigation</b>:
        1. Unit test each extractor (test on 5 papers with manual validation)
        2. Batch processing: Extract, save intermediate, validate each batch
        3. Feature validation script: Check for outliers (>3œÉ), missing values, data types
        4. Version control all extraction code
        5. Automated tests run on every code change
        
        <b>Success Metric</b>: 100% test coverage for feature extraction module
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 3: Statistical Issues
    with challenge_tabs[2]:
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Proportional Odds Assumption</h4>
        
        <b>Problem</b>: Real data may violate proportional odds assumption (Œ≤ same across thresholds)
        
        <b>Impact</b>: Model misspecification; incorrect confidence intervals
        
        <b>Mitigation</b>:
        1. Test assumption for all features (Brant test, Œ±=0.05)
        2. If violated: Use partial proportional odds model (relax for that feature)
        3. Or: Use robust standard errors (bootstrapped)
        4. Document which assumptions violated + how addressed
        
        <b>Success Metric</b>: Brant test p>0.05 for ‚â•70% of features
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Multicollinearity</h4>
        
        <b>Problem</b>: Features correlated (VIF >5). Example: stat density & test count (r=0.72)
        
        <b>Impact</b>: Unstable coefficient estimates; overfitting
        
        <b>Mitigation</b>:
        1. Calculate VIF for all features
        2. Remove features with VIF >5 (keep simpler one)
        3. Or: Use regularization (Ridge/Lasso)
        4. Or: PCA to create orthogonal combinations
        5. Monitor: Coefficients should be stable across CV folds
        
        <b>Success Metric</b>: All VIF <5, coefficient SE <0.5
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Small Validation Set (n=50)</h4>
        
        <b>Problem</b>: Only 50 validation papers. High variance in metrics.
        
        <b>Impact</b>: Reported accuracy could be ¬±10% due to random variation
        
        <b>Mitigation</b>:
        1. Bootstrap confidence intervals on all metrics
        2. Report both point estimates and CIs
        3. 5-fold cross-validation on training set
        4. Monitor: Metrics should be stable across folds
        5. Be conservative in claims
        
        <b>Success Metric</b>: CI width <0.08 (e.g., [0.70, 0.78])
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 4: NLP & Retrieval
    with challenge_tabs[3]:
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Irrelevant Retrievals</h4>
        
        <b>Problem</b>: Retrieved papers may be topically similar but methodologically different.
        
        <b>Impact</b>: Retrieved examples mislead; poor quality feedback
        
        <b>Mitigation</b>:
        1. Entity filtering (NER): Ensure similar statistical methods
        2. Mahalanobis filtering: Ensure statistical profiles match
        3. Manual spot-check: For 10 queries, verify retrieved papers appropriate
        4. Set stricter threshold if needed (keep top-5 instead of top-10)
        5. Document retrieval quality metric
        
        <b>Success Metric</b>: ‚â•70% of retrieved papers rated "relevant" by manual review
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Embedding Quality for Domain Concepts</h4>
        
        <b>Problem</b>: S-PubMedBert may conflate nuanced concepts.
        Example: "propensity score matching" vs "matching"
        
        <b>Impact</b>: Relevant papers missed in semantic search
        
        <b>Mitigation</b>:
        1. Hybrid retrieval: Combine dense (embeddings) + sparse (BM25 keywords)
        2. Test embedding quality: Query 20 papers, check if top-5 are relevant
        3. If poor: Fine-tune embeddings on domain data (requires labeled pairs)
        4. Use multiple embedding models + ensemble (slower but robust)
        
        <b>Success Metric</b>: ‚â•75% of top-5 retrievals rated "relevant"
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: LLM Hallucination (if used)</h4>
        
        <b>Problem</b>: LLM may generate plausible-sounding but false claims
        
        <b>Impact</b>: Misleading recommendations; reduced trust
        
        <b>Mitigation</b>:
        1. Constrain LLM: System prompt forbids hallucination
        2. Verify: Check if claims are supported by retrieved text (NLI verification)
        3. Fallback: If LLM output inconsistent with statistical model, don't trust it
        4. Be transparent: Always show retrieved papers + evidence
        
        <b>Success Metric</b>: 100% of recommendations grounded in retrieved evidence
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 5: Integration
    with challenge_tabs[4]:
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Ensemble Weight Calibration</h4>
        
        <b>Problem</b>: Computing weights on 50 papers is unstable. Weights change if validation set changes.
        
        <b>Impact</b>: Final predictions vary based on which papers in holdout
        
        <b>Mitigation</b>:
        1. Use multiple evaluation metrics (accuracy, F1, Brier). Weight should balance all.
        2. 5-fold cross-validation: Get stable weight estimates
        3. Report weight uncertainty: "w_M1 = 0.33 ¬± 0.05"
        4. Keep weights fixed after training; don't retune on new data
        
        <b>Success Metric</b>: Weight SEM <0.02 across CV folds
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Inference Latency</h4>
        
        <b>Problem</b>: Cross-encoder re-ranking is 10x slower than bi-encoder (30 sec for 50 candidates)
        
        <b>Impact</b>: Total inference time >5 seconds (target: <2 sec)
        
        <b>Mitigation</b>:
        1. Only re-rank top-50 from bi-encoder (not all 1500)
        2. Use smaller cross-encoder (MiniLM-L-12 instead of base)
        3. Batch processing (32 candidates at a time)
        4. Cache: Store cross-encoder scores for training set (reuse on inference)
        5. Optional: Skip re-ranking if bi-encoder confidence already high
        
        <b>Success Metric</b>: Total latency <3 seconds (M1+M2<500ms, M3 ~2500ms)
        </div>
        """, unsafe_allow_html=True)
    
    # Tab 6: Timeline Risk
    with challenge_tabs[5]:
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Feature Extraction Takes Longer Than Expected</h4>
        
        <b>Risk</b>: One complex feature (NLP-based) takes 2-3 weeks instead of planned
        
        <b>Mitigation</b>:
        1. Build features in order of complexity (regex-based first, NLP-based later)
        2. If feature taking too long: Simplify or drop it
        3. Have "optional" feature list (can skip if needed)
        4. Time-box: Spend max 5 days per feature; if not working, defer
        5. Progress check: Week 6 should have ‚â•10 features working
        
        <b>Contingency</b>: Reduce from 15 to 12 features if needed (still valid)
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="challenge-box">
        <h4>Challenge: Model Accuracy Below Target (<70%)</h4>
        
        <b>Risk</b>: Ensemble only achieves 68% despite careful engineering
        
        <b>Mitigation</b>:
        1. Add more features? (if time permits)
        2. Improve retrieval quality? (hyperparameter tuning)
        3. Accept lower accuracy: Document as limitation
        4. Acknowledge: 68% still useful for authors (vs 33% random)
        5. Focus thesis on methodological contribution, not just accuracy
        
        <b>Contingency</b>: Already have 5 weeks buffer (Weeks 16-20) for improvements
        </div>
        """, unsafe_allow_html=True)

# ============================================================================
# PAGE 6: EXPECTED OUTCOMES
# ============================================================================

elif page == "üìà Expected Outcomes":
    st.markdown('<div class="main-header">üìà Expected Outcomes & Targets</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    ## Primary Outcomes
    
    ### Performance Targets
    """)
    
    performance_targets = pd.DataFrame({
        'Metric': [
            'Overall Accuracy',
            'Macro F1-Score',
            'Brier Score (Calibration)',
            'Bootstrap CI Coverage',
            'Feature Importance (Top Feature)',
            'Model Inference Time',
            'RAG Retrieval Time'
        ],
        'Target': [
            '75-80%',
            '‚â•0.70',
            '<0.05',
            '93-97%',
            'Coefficient p<0.001',
            '<500ms',
            '<3 seconds'
        ],
        'Justification': [
            'Based on ordinal models in similar domains',
            'Balanced performance across 3 decision classes',
            'Excellent calibration (< 0.05 is gold standard)',
            'Bootstrap CIs should have proper coverage',
            'At least one feature significantly predicts decision',
            'Real-time feedback for users',
            'Practical for batch processing'
        ],
        'Risk Level': [
            'Low-Medium',
            'Low-Medium',
            'Low',
            'Low',
            'Very Low',
            'Very Low',
            'Medium'
        ]
    })
    
    st.dataframe(performance_targets, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # Expected Performance by Decision Class
    st.subheader("Expected Performance by Decision Class")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        performance_by_class = pd.DataFrame({
            'Decision': ['Accept', 'Minor Revision', 'Major Revision'],
            'Accuracy': [0.81, 0.73, 0.75],
            'Precision': [0.81, 0.73, 0.75],
            'Recall': [0.78, 0.70, 0.77],
            'F1-Score': [0.79, 0.71, 0.76]
        })
        
        fig = go.Figure()
        
        for metric in ['Precision', 'Recall', 'F1-Score']:
            fig.add_trace(go.Bar(
                x=performance_by_class['Decision'],
                y=performance_by_class[metric],
                name=metric
            ))
        
        fig.update_layout(
            title='Expected Performance Metrics by Decision Class',
            xaxis_title='Decision Type',
            yaxis_title='Score',
            barmode='group',
            height=400,
            template='plotly_white'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("""
        **Reasoning**:
        
        - **Accept** (20-25% of papers)
          - Higher precision due to clear quality signals
          - ~4/5 correct
        
        - **Minor** (30-35% of papers)
          - Most difficult to predict (borderline cases)
          - Similar accuracy to Major
        
        - **Major** (40-45% of papers)
          - Clear signal (many issues)
          - ~3/4 correct
        
        **Insight**: Order accuracy ‚â† class difficulty
        """)
    
    st.markdown("---")
    
    # Feature Importance Predictions
    st.subheader("Expected Feature Importance (Top 10)")
    
    st.markdown("""
    Based on peer review literature, we expect these features to be most predictive:
    """)
    
    expected_features = pd.DataFrame({
        'Rank': list(range(1, 11)),
        'Feature': [
            'Effect Size Reporting',
            'Statistical Term Density',
            'Named Statistical Test Count',
            'Sample Size Justified',
            'Readability (Flesch-Kincaid)',
            'Ethics Approval Statement',
            'Citation Recency (Median Year)',
            'Methods Section Ratio',
            'Passive Voice Ratio',
            'Data Availability Statement'
        ],
        'Expected Coefficient': [-1.2, -0.9, -0.8, -0.6, -0.35, -0.3, 0.1, 0.15, 0.12, -0.25],
        'Directional Effect': [
            'Lower odds of worse decision',
            'Lower odds of worse decision',
            'Lower odds of worse decision',
            'Lower odds of worse decision',
            'Lower odds of worse decision',
            'Lower odds of worse decision',
            'Mixed effect',
            'Higher odds of worse decision',
            'Higher odds of worse decision',
            'Lower odds of worse decision'
        ]
    })
    
    st.dataframe(expected_features, use_container_width=True, hide_index=True)
    
    st.markdown("""
    **Backing**:
    - Effect sizes: Publication Manual (APA), Fisher et al. 2019
    - Statistical reporting: STROBE guidelines (Vandenbroucke et al. 2007)
    - Readability: Flesch 1948, Gunning 1952
    - Ethics: CONSORT guidelines (Schulz et al. 2010)
    - Citation recency: Field-specific citation patterns
    """)
    
    st.markdown("---")
    
    # Secondary Outcomes
    st.subheader("Secondary Outcomes & Outputs")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### Research Contributions
        
        ‚úì **First integration of RAG + ordinal regression for peer review**
        - Novel methodological approach
        - Shows RAG improves over baseline by ~2-4%
        
        ‚úì **Interpretable decision explanations**
        - Feature importance breakdowns
        - Retrieved evidence for each decision
        - Actionable recommendations
        
        ‚úì **Calibrated uncertainty quantification**
        - Bootstrap confidence intervals
        - Reliable calibration curves
        - Honest about when model is uncertain
        
        ‚úì **Public dataset + code**
        - 250 PeerJ papers with annotations
        - Reproducible feature extraction
        - Open-source implementation
        """)
    
    with col2:
        st.markdown("""
        ### Practical Impact
        
        **For Authors**:
        - Get early feedback before submission
        - Identify specific weaknesses
        - ~+20% improvement if recommendations followed
        
        **For Journals**:
        - Data-driven editorial decisions
        - Detect reviewer bias
        - Optimize review assignments
        
        **For Community**:
        - Blueprint for similar applications
        - Advance in scientific publishing
        - Enable research on peer review bias
        
        **Publications**:
        - Peer-reviewed journal paper
        - Software preprint
        - Code + dataset on GitHub
        """)
    
    st.markdown("---")
    
    st.subheader("Realistic Risk Assessment")
    
    col1, col2 = col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        ### Best Case (30% probability)
        
        ‚úì 78-80% accuracy achieved
        ‚úì All features work as expected
        ‚úì RAG improves by 3-5%
        ‚úì Model generalizes to validation set
        ‚úì Published in peer-reviewed journal
        """)
    
    with col2:
        st.markdown("""
        ### Expected Case (50% probability)
        
        ‚úì 75-77% accuracy achieved
        ‚úì 12-14 features work well
        ‚úì RAG improves by 1-2%
        ‚úì Some generalization loss
        ‚úì Strong thesis, submitted to journal
        """)
    
    with col3:
        st.markdown("""
        ### Worst Case (20% probability)
        
        ‚úì 70-73% accuracy achieved
        ‚úì 10-12 features work
        ‚úì RAG minimal improvement
        ‚úì Limited generalization
        ‚úì Thesis submitted, negative results
        """)

# ============================================================================
# PAGE 7: EVALUATION PLAN
# ============================================================================

elif page == "üìã Evaluation Plan":
    st.markdown('<div class="main-header">üìã Evaluation & Validation Plan</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    ## Comprehensive Evaluation Strategy
    
    We will use multiple evaluation approaches to ensure robustness and generalizability.
    """)
    
    st.markdown("---")
    
    eval_tabs = st.tabs([
        "Metrics & Thresholds",
        "Validation Procedure",
        "Failure Analysis",
        "Generalization Testing",
        "Reproducibility"
    ])
    
    # Tab 1: Metrics
    with eval_tabs[0]:
        st.markdown("""
        ### Classification Metrics
        
        **Primary Metric**: Macro F1-Score (target ‚â•0.70)
        - Equal weight per class (fair for imbalanced data)
        - Considers both precision and recall
        - Backed by: Scikit-learn best practices
        
        **Secondary Metrics**:
        """)
        
        metrics = pd.DataFrame({
            'Metric': [
                'Accuracy',
                'Precision (per class)',
                'Recall (per class)',
                'Weighted F1',
                'Confusion Matrix'
            ],
            'Formula': [
                'TP+TN / Total',
                'TP / (TP+FP)',
                'TP / (TP+FN)',
                'Avg weighted by class frequency',
                'Shows misclassification patterns'
            ],
            'Interpretation': [
                'Overall % correct',
                'When model predicts class X, how often right?',
                'Of all actual X, how many detected?',
                'Weighted by actual distribution',
                'Where does model make mistakes?'
            ],
            'Target': [
                '‚â•75%',
                '‚â•0.70',
                '‚â•0.70',
                '‚â•0.75',
                'No > 20% off-diagonal elements'
            ]
        })
        
        st.dataframe(metrics, use_container_width=True, hide_index=True)
        
        st.markdown("""
        ### Calibration Metrics
        
        **Primary Metric**: Brier Score (target <0.05)
        - BS = (1/n) Œ£ (p_predicted - y_actual)¬≤
        - <0.02: Excellent
        - 0.02-0.05: Good
        - 0.05-0.10: Fair
        - >0.10: Poor
        
        **Backed by**: Proper scoring rules (Gneiting & Raftery 2007)
        """)
        
        st.markdown("""
        ### Uncertainty Metrics
        
        **Bootstrap Confidence Interval Coverage**:
        - Generate 1000 bootstrap resamples
        - For each sample, train new model
        - Get confidence interval bounds
        - On validation set: Check if true decision falls within CI ~95% of time
        - Target: 93-97% coverage (proper uncertainty)
        """)
    
    # Tab 2: Validation Procedure
    with eval_tabs[1]:
        st.markdown("""
        ### Data Split Strategy
        
        | Set | Size | Purpose | Selection |
        |-----|------|---------|-----------|
        | Training | 200 (80%) | Fit models M1, M2, RAG DB | Random stratified |
        | Validation | 50 (20%) | Evaluate, compute ensemble weights, calibration | Random stratified |
        | Cross-Validation | All 250 | Assess stability, generalization | 5-fold stratified |
        
        **Stratification**: Ensure decision distribution matches across sets
        """)
        
        st.markdown("""
        ### Validation Procedures
        
        **1. Hold-Out Validation** (Week 13)
        - Train on 200 papers
        - Evaluate on 50 held-out papers
        - Compute: Accuracy, F1, Brier, calibration
        
        **2. k-Fold Cross-Validation** (Week 14)
        - 5-fold CV on all 250 papers
        - Train 5 models (each on 80%, test on 20%)
        - Compute: Mean accuracy, SD across folds
        - Interpretation: How stable is model?
        - Backed by: Kohavi 1995, recommended practice
        
        **3. Bootstrap Resampling** (Week 15)
        - 1000 bootstrap resamples of training set
        - Compute: CI on coefficients, accuracy, metrics
        - Interpretation: Uncertainty in estimates
        - Backed by: Efron 1979
        
        **4. Feature Validation** (Week 6)
        - Manual extraction on 20 papers
        - Compare: Automated vs manual
        - Cohen's kappa target: >0.90
        - Identify problematic features
        
        **5. Study-Type Validation** (Week 8)
        - Calculate norms separately per study type
        - Verify: RCT norms differ from Cohort norms
        - Ensure: Features make sense per type
        
        **6. Temporal Validation** (Optional)
        - If data spans 2015-2024, test on recent papers (2023-2024)
        - Interpretation: Do standards change over time?
        - May require: Separate recent model
        """)
        
        st.markdown("""
        ### Calibration Testing (Week 13)
        
        **Procedure**:
        1. Predictions on validation set (50 papers)
        2. Bin predictions: [0-0.1], [0.1-0.2], ..., [0.9-1.0]
        3. For each bin, calculate: (# predicted class) / (total in bin)
        4. Plot: Predicted % vs Observed %
        5. Perfect calibration: Points on diagonal line
        
        **If miscalibrated**:
        - Platt scaling: Fit logistic regression on outputs
        - Isotonic regression: Non-parametric calibration
        - Confidence penalty: Report wider CIs
        """)
    
    # Tab 3: Failure Analysis
    with eval_tabs[2]:
        st.markdown("""
        ### Error Analysis Strategy
        
        **Question 1**: What types of errors does model make?
        
        Error patterns to investigate:
        - Major ‚Üí Minor (overly optimistic)
        - Major ‚Üí Accept (rare, model confused)
        - Minor ‚Üí Major (overly pessimistic)
        - Accept ‚Üí Major (rare, model confused)
        
        For each error: Analyze features
        - Which feature values led to error?
        - Do retrieved papers support actual decision?
        - Model confidence: Was model certain about wrong prediction?
        """)
        
        st.markdown("""
        ### Confidence in Errors
        
        **Insight**: Model makes different errors with different confidence
        
        Examples:
        - Error with 52% confidence ‚Üí Expected (hard decision)
        - Error with 85% confidence ‚Üí Problem (model overconfident)
        
        **Procedure**:
        1. Separate correct vs incorrect predictions
        2. Compare confidence distributions
        3. Identify: Are high-confidence errors rare? (desired)
        """)
        
        st.markdown("""
        ### Feature-Level Error Analysis
        
        For each misclassified paper:
        1. Which features triggered the error?
        2. Were critical features marked correctly?
        3. Was it a feature extraction error or model error?
        
        Example:
        - Paper predicted "Major" but actually "Minor"
        - Check: Was "effect_size" correctly extracted?
        - If yes: Model made error
        - If no: Feature extraction error
        """)
    
    # Tab 4: Generalization Testing
    with eval_tabs[3]:
        st.markdown("""
        ### Cross-Domain Generalization
        
        **Limitation**: Model trained on PeerJ only
        
        **Testing Plan** (if time permits):
        
        1. **Across journals** (Week 20)
           - Find 10 papers from different journal (if available)
           - Test: Does model work on different review standards?
           - Expected: Performance drop of 10-20%
        
        2. **Across study types** (Built-in)
           - Ensure sufficient papers per study type
           - Calculate: Accuracy by RCT vs Cohort vs Cross-sectional
           - Expected: Within 3-5% of each other
        
        3. **Across time periods** (Built-in)
           - Split by year: 2015-2018 vs 2019-2024
           - Calculate: Accuracy on recent papers
           - Expected: Similar or slightly lower (standards may change)
        
        4. **Across language complexity**
           - Split by Flesch-Kincaid grade
           - Expected: Accuracy similar across reading levels
        """)
        
        st.markdown("""
        ### Robustness Testing
        
        1. **Outlier handling**
           - Papers with very short/long methods
           - Expected: Model should flag as "uncertain"
        
        2. **Missing data**
           - Papers missing certain sections
           - Expected: Model handles gracefully
        
        3. **Domain shift**
           - Papers outside training distribution
           - Expected: Model reports low confidence
        """)
    
    # Tab 5: Reproducibility
    with eval_tabs[4]:
        st.markdown("""
        ### Reproducibility Checklist
        
        ‚úì **Code reproducibility**
        - All code version controlled (GitHub)
        - Requirements.txt with exact versions
        - Docker image for reproducible environment
        - Seeds set for random number generation
        - Documentation of hyperparameters
        
        ‚úì **Data reproducibility**
        - Original PeerJ papers archived (read-only)
        - Feature extraction logs saved
        - Intermediate results version controlled
        - Statistical reference distributions documented
        
        ‚úì **Results reproducibility**
        - All results tables + figures saved
        - Statistical tests documented (which p-value, test statistic)
        - Model weights + architecture saved
        - Validation procedure fully documented
        
        ‚úì **Documentation**
        - API documentation (docstrings)
        - README with setup + usage instructions
        - Feature codebook with examples
        - Model specifications (equations, assumptions)
        - Evaluation protocol detailed
        """)
        
        st.markdown("""
        ### Open Science Practices
        
        At project completion:
        
        1. **Code Release**
           - GitHub repository (open-source)
           - CI/CD pipeline (tests on commit)
           - Releases with version tags
           - MIT License
        
        2. **Dataset Release**
           - 250 papers with feature vectors + decisions
           - Anonymized (remove author names)
           - Hosted on OSF or Figshare
           - DOI for citation
        
        3. **Preprints**
           - arXiv: Full thesis or paper
           - bioRxiv: If biological focus
        
        4. **Peer Review**
           - Submit to NLP + ML venue (ACL, EMNLP)
           - Or: Domain venue (JMIR, PLoS Medicine)
        
        **Backed by**: FAIR principles (Findable, Accessible, Interoperable, Reusable)
        """)

# ============================================================================
# FOOTER
# ============================================================================

st.markdown("---")

footer_col1, footer_col2, footer_col3 = st.columns(3)

with footer_col1:
    st.markdown("""
    **Project Status**: Pre-Implementation (Pitch Phase)
    
    **Last Updated**: December 7, 2025
    """)

with footer_col2:
    st.markdown("""
    **Duration**: 24 weeks
    
    **Expected Completion**: Mid-2026
    """)

with footer_col3:
    st.markdown("""
    **Contact**: [Your Name]
    
    **Email**: [Your Email]
    """)
